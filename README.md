# Literature:



** Denotes highly relevant paper
## [Content](#content)

<table>
<tr><td colspan="2"><a href="#cognitive-science">1. Cognitive Science</a></td></tr> 
<tr><td colspan="2"><a href="#state-representation-and-abstraction">2. State Representation and Abstraction</a></td></tr>
<tr><td colspan="2"><a href="#temporal-and-state-abstraction"> 2.1 Temporal and state abstraction</a></td></tr>
<tr><td colspan="2"><a href="#symbolic-representation-and-planning"> 2.2 Symbolic Representation and planning</a></td></tr>
<tr><td colspan="2"><a href="#hierarchical-reinforcement-learning">3. Hierarchical Reinforcement Learning</a></td></tr>
<tr><td colspan="2"><a href="#gnnrl">4. GNN+RL</a></td></tr>
<tr><td colspan="2"><a href="#various">5. Various</a></td></tr>
<tr><td colspan="2"><a href="#read-later">6. Read later</a></td></tr>

</table>

## [Cognitive Science](#content)
1. **Reward is enough, 2021** [Link](https://www.sciencedirect.com/science/article/pii/S0004370221000862)


    *David Silver, Satinder Singh, Doina Precup, Richard S.Sutton* 
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>


2. **Hierarchical principles of embodied reinforcement learning: A review, 2022** [Link](https://arxiv.org/abs/2012.10147)

    *Manfred Eppe, Christian Gumbsch, Matthias Kerzel, Phuong D.H. Nguyen, Martin V. Butz, Stefan Wermter* 
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>
    
    
## [State Representation and Abstraction](#content)

1. **Planning to Explore via Self-Supervised World Models, 2022** [Link](https://arxiv.org/abs/2005.05960)

    *Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, Deepak Pathak* 
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>

2. **Reinforcement Learning with Prototypical Representations, 2021** [Link](https://arxiv.org/abs/2102.11271)

     *Denis Yarats, Rob Fergus, Alessandro Lazaric, Lerrel Pinto* 
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> I have been able to replicate the results. </a>
  </ol>
</details>

    
 3. **Near Optimal Behavior via Approximate State Abstraction, 2016** [Link](https://arxiv.org/abs/1701.04113)

    *David Abel, D. Ellis Hershkowitz, Michael L. Littman* 
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>

 4. **On the necessity of abstraction, 2019** [Link](https://www.sciencedirect.com/science/article/pii/S2352154618302080)

    *George Konidaris* 
    
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> Opinion paper </a>
  </ol>
</details>
    
5. **The value of abstraction, 2016** [Link](https://markkho.github.io/documents/cobs2019_value_of_abstr.pdf)

    *Mark K. Ho, David Abel, Thomas L. Griffiths and Michael L. Littman* 
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> Opinion paper </a>
  </ol>
</details>
        
6. **A Theory of Abstraction in Reinforcement Learning, 2020** [Link](https://david-abel.github.io/thesis.pdf)

    *David Abel* 
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> PhD Thesis </a>
  </ol>
</details>
    
7. **Learning Markov State Abstractions for Deep Reinforcement Learning, 2021** [Link](https://arxiv.org/abs/2106.04379) **

    *Cameron Allen, Neev Parikh, Omer Gottesman, George Konidaris* 
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>

8. **State Abstractions for Lifelong Reinforcement Learning, 2018** [Link](https://arxiv.org/abs/2106.04379) **

    *David Abel, Dilip Arumugam, Lucas Lehnert, Michael L. Littman* 

<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>

9. **DeepMDP: Learning Continuous Latent Space Models for Representation Learning, 2019** [Link](https://arxiv.org/pdf/1906.02736.pdf) 

    *Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, Marc G. Bellemare* 
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>

10. **Learning Invariant Representations for Reinforcement Learning without Reconstruction, 2019** [Link](https://arxiv.org/abs/2006.10742) 

    *Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, Sergey Levine* 
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>

11. **Reinforcement Learning with Soft State Aggregation, 1994** [Link](https://proceedings.neurips.cc/paper/1994/file/287e03db1d99e0ec2edb90d079e142f3-Paper.pdf) 

    *Satinder Singh, Tommi Jaakkola, Michael Jordan* 
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> 
The FA maps the state space S into M > 0 aggregates or clusters from cluster space X. Typically, M < < S. We allow soft clustering, where each state s belongs to cluster x with
probability P(xls), called the clustering probabilities. This allows each state s to
belong to several clusters </a>
  </ol>
</details>





### [Temporal and state abstraction](#content)
      
 1. **Temporal Abstraction in Reinforcement Learning with the Successor Representation, 2022** [Link](https://arxiv.org/abs/2110.05740) **

    *Marlos C. Machado, Andre Barreto, Doina Precup, Michael Bowling*

<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>

 2. **Discrete State-Action Abstraction via the Successor Representation, 2023** [Link](https://arxiv.org/abs/2206.03467)

    *Amnon Attali, Pedro Cisneros-Velarde, Marco Morales, Nancy M. Amato*
 
 <details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>

3. **Deep Laplacian-based Options for Temporally-Extended Exploration, 2023** [Link](https://arxiv.org/pdf/2301.11181.pdf)

    *Martin Klissarov, Marlos C. Machado*
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>

4. **Successor Options: An Option Discovery Framework for Reinforcement Learning, 2019** [Link](https://arxiv.org/pdf/1905.05731.pdf)

    *Manan Tomar, Rahul Ramesh, Balaraman Ravindran*
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>

5. **Context-Specific Representation Abstraction for Deep Option Learning, 2022** [Link](https://arxiv.org/pdf/2109.09876.pdf)

    *Marwa Abdulhai, Dong-Ki Kim, Matthew Riemer, Miao Liu, Gerald Tesauro, Jonathan P. How*
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>
    
    
6. **Hierarchical reinforcement learning for efficient exploration and transfer, 2020** [Link](https://arxiv.org/pdf/2011.06335.pdf) **

    *Lorenzo Steccanella, Simone Totaro, Damien Allonsius, Anders Jonsson*
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> In this work, an invariant state space that is common to a range of tasks is compressed into a partition. It's assumed that the agent has access to the partition. </a>
  </ol>
</details>

7. **Near-Optimal Representation Learning for Hierarchical Reinforcement Learning, 2019** [Link](https://arxiv.org/abs/1810.01257) **

    *Ofir Nachum, Shixiang Gu, Honglak Lee, Sergey Levine*
    
<details>
  <summary> Comments </summary>
  <ol>
    <p> Hierarchical framework: 
       1. $\pi_{hi}$ decides a goal state space $g$. 
       2. Knowing $g$, we obtain the low level policy by function $\Psi$, which depends on a representation function $f$. 3. We repeat this every $c$ steps.
     They study the sub-obtimality bounds of this approach under a given representation. They propose a representation learning objective to learn this representation. 
     </p>
     <p> 
       Remarks: I think it is a nice paper, very related to my interests and research ideas. I don't like the bounds they achive, being a bit unclear to me the use of $\phi$
     </p> 
     <p>   
     Code at https://github.com/tensorflow/models/tree/master/research/efficient-hrl (Not updated)
     </p>
     
  </ol>
</details>


### [Symbolic Representation and planning](#content)

1. **Learning State and Action Abstractions for Effective and Efficient Planning, 2020** [Link](https://dspace.mit.edu/handle/1721.1/145150)

    *Rohan Chitnis* 
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>

2. **From Skills to Symbols: Learning Symbolic Representations for Abstract High-Level Planning, 2018** [Link](https://cs.brown.edu/people/gdk/pubs/orig_sym_jair.pdf)

    *George Konidaris, Leslie Pack Kaelbling, Tomas Lozano-Perez*
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>

3. **Discovering State and Action Abstractions for Generalized Task and Motion Planning, 2022** [Link](https://cs.brown.edu/people/gdk/pubs/orig_sym_jair.pdf)

    *Aidan Curtis, Tom Silver, Joshua B. Tenenbaum, Tomas Lozano-PÃ©rez, Leslie Kaelbling*
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>

## [Hierarchical Reinforcement Learning](#content)

1. **Between MDPs and Semi-MDPs: Learning, Planning, and Representing Knowledge at Multiple Temporal Scales, 1999** [Link](https://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1212&context=cs_faculty_pubs)

    *Richard Sutton, Doina Precup, Satinder Singh* 
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>    

2. **Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition, 2000** [Link](https://arxiv.org/pdf/cs/9905014.pdf)

    *Tom Dietterich* 
    
<details>
  <summary> Comments </summary>
  <ol>
    <p> Related to other work from Dietterich: State Abstraction in MAXQ Hierarchical Reinforcement Learning. The author proposes 5 conditions for Safe State Abstraction (ie. under which a specific RL algorithm operating in the MAXQ framework is guaranteed to discovery a globally optimal policy). Framework: each state represented by a vector of variables. Abstraction in this case means how to discard useless variables without any loss.   </p>
    <p> Condition 1: Subtask Irrelevance. Let $M_i$ be a subtask of MDP $M$. A set of state variables $Y$ is irrelevant to subtask $i$ if the state variables of $M$ can be partitioned into two sets $X$ and $Y$ such that for any stationary abstract hierarchical policy $\pi$ executed by the descendants of $M_i$, the following two properties hold: (a) the state transition probability distribution $P^\pi\left(s^{\prime}, N \mid s, j\right)$ for each child action $j$ of $M_i$ can be factored into the product of two distributions:
$$P^\pi\left(x^{\prime}, y^{\prime}, N \mid x, y, j\right)=P^\pi\left(x^{\prime}, N \mid x, j\right) \cdot P^\pi\left(y^{\prime} \mid x, y, j\right)$$
where $x$ and $x^{\prime}$ give values for the variables in $X$, and $y$ and $y^{\prime}$ give values for the variables in $Y$; and (b) for any pair of states $s_1=\left(x, y_1\right)$ and $s_2=\left(x, y_2\right)$ and any child action $j, V^\pi\left(j, s_1\right)=V^\pi\left(j, s_2\right)$. </p>
     <p>Condition 2: Leaf Irrelevance. A set of state variables Y is irrelevant for a primitive action a if for any pair of states s1 and s2 that differ only in their values for the variables in Y, the expected reward is the same. </p>
     <p>Condition 3: Result Distribution Irrelevance (Undiscounted case.) A set of state variables Y is irrelevant for the result distribution of action if, for all abstract policies pi executed by M j and its descendants in the MAXQ hierarchy, the following holds: for all pairs of states s1 and s2 that differ only in their values for the state variables in Y, the probability of finding s given policy pi is equal for s1 and s2. Note that this is only true in the undiscounted settingwith discounting, the result distributions are not the same because the number of
steps N required to reach s might depend on the starting state. Hence this form of state abstraction is rarely useful for cumulative discounted
reward.</p>
     <p>Condition 4: Termination. Every child task $M_{j}$ whose termination implies the termination of the main task, M, can be abstracted into terminal state of M. </p>
     <p>Condition 5: Shielding. Basically means you don't need to represent in s subtasks that are inaccessible from s</p>
  </ol>
</details>    

3. **The Promise of Hierarchical Reinforcement Learning, 2019** [Link](https://thegradient.pub/the-promise-of-hierarchical-reinforcement-learning/)

    *Yannis Flet-Berliac*
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>    

4. **The Option-Critic Architecture, 2016** [Link](https://arxiv.org/abs/1609.05140)

    *Pierre-Luc Bacon, Jean Harb, Doina Precup*

<details>
  <summary> Comments </summary>
  <ol>
    <a> I have been able to replicate the results using the pytorch implementation available here:  [Link](https://github.com/lweitkamp/option-critic-pytorch). I've implemented in the pacman environment as well. The results for Pacman are not good yet. </a>
  </ol>
</details>
    

    
5. **HIERARCHICAL REINFORCEMENT LEARNING IN CONTINUOUS STATE AND MULTI-AGENT ENVIRONMENTS, 2005** [Link](http://all.cs.umass.edu/pubs/2005/ghavamzadeh_thesis05.pdf)

    *MOHAMMAD GHAVAMZADEH*
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> PhD thesis </a>
  </ol>
</details> 
    
6. **Induction and Exploitation of Subgoal Automata for Reinforcement Learning, 2021** [Link](https://arxiv.org/pdf/2009.03855.pdf)

    *Daniel Furelos-Blanco, Mark Law, Anders Jonsson, Krysia Broda, Alessandra Russo*
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>    

7. **The Option Keyboard: Combining Skills in Reinforcement Learning, 2021** [Link](https://arxiv.org/pdf/2009.03855.pdf)

    *AndrÃ© Barreto, Diana Borsa, Shaobo Hou, Gheorghe Comanici, Eser AygÃ¼n, Philippe Hamel, Daniel Toyama, Jonathan Hunt, Shibl Mourad, David Silver, Doina Precup*
    
<details>
  <summary> Comments </summary>
  <ol>
    <p> They propose a framework for combining skills using the formalism of options. They also show how to approximate options whose cumulants are
linear combinations of the cumulants of known options. </p>
    <p> Problem: understand the concept of cumulants and how they use them <p>
  </ol>
</details> 
    
    

## [GNN+RL](#content)
1. **NerveNet: Learning Structured Policy with Graph Neural Networks , 2018**[Link](https://openreview.net/forum?id=S1sqHMZCb)

*Tingwu Wang, Renjie Liao, Jimmy Ba, Sanja Fidler*

<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details> 

2. **Hierarchical Representations and Explicit Memory: Learning Effective Navigation Policies on 3D Scene Graphs using Graph Neural Networks, 2021** [Link](https://arxiv.org/abs/2108.01176)

    *Zachary Ravichandran, Lisa Peng, Nathan Hughes, J. Daniel Griffith, Luca Carlone* 
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>    

3. **Reward Propagation Using Graph Convolutional Networks, 2020** [Link](https://proceedings.neurips.cc/paper/2020/file/970627414218ccff3497cb7a784288f5-Paper.pdf)

    *Martin Klissarov, Doina Precup* 
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details> 
    
    

## [Various](#content)


## [Read later](#content)

Provably efficient RL with rich observations via latent state decoding + Kinematic state abstraction and provably efficient rich-observation reinforcement learning


**Regret bounds for learning state representations in reinforcement learning.**

**Automatic construction of temporally extended actions for MDPs using bisimulation metrics**

**Abstract Value Iteration for Hierarchical Reinforcement Learning**

**On the Geometry of Reinforcement Learning in Continuous State and Action Spaces**

**Option Transfer and SMDP Abstraction with Successor Features**

**Learning Causal State Representations of Partially Observable Environments**

**Efficient Reinforcement Learning in Block MDPs: A Model-free Representation Learning Approach**

**Learning Invariant Representations for Reinforcement Learning without Reconstruction**

**Towards a Unified Theory of State Abstraction for MDPs**

**CONTRASTIVE LEARNING OF STRUCTURED WORLD MODELS**

**Continual curiosity-driven skill acquisition from high-dimensional video inputs for humanoid robots**

**Notes on State Abstractions, Nan Jiang**

