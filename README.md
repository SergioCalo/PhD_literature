# Literature:
Use this space to write new references (link to the paper, name, etc.):



## [Content](#content)

<table>
<tr><td colspan="2"><a href="#cognitive-science">1. Cognitive Science</a></td></tr> 
<tr><td colspan="2"><a href="#state-representation-learning">2. State Representation Learning</a></td></tr>
<tr><td colspan="2"><a href="#hierarchical-reinforcement-learning">3. Hierarchical Reinforcement Learning</a></td></tr>
<tr><td colspan="2"><a href="#gnnrl">4. GNN+RL</a></td></tr>
<tr><td colspan="2"><a href="#various">5. Various</a></td></tr>

</table>

## [Cognitive Science](#content)
1. **Reward is enough, 2021** [Link](https://www.sciencedirect.com/science/article/pii/S0004370221000862)

    *David Silver, Satinder Singh, Doina Precup, Richard S.Sutton* 

2. **Hierarchical principles of embodied reinforcement learning: A review, 2022** [Link](https://arxiv.org/abs/2012.10147)

    *Manfred Eppe, Christian Gumbsch, Matthias Kerzel, Phuong D.H. Nguyen, Martin V. Butz, Stefan Wermter* 

    
    
## [State Representation Learning](#content)

1. **Planning to Explore via Self-Supervised World Models, 2022** [Link](https://arxiv.org/abs/2005.05960)

    *Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, Deepak Pathak* 
    
2. **Reinforcement Learning with Prototypical Representations, 2021** [Link](https://arxiv.org/abs/2102.11271)

    *Denis Yarats, Rob Fergus, Alessandro Lazaric, Lerrel Pinto* 
    
    Notes: I have been able to replicate the results.


## [Hierarchical Reinforcement Learning](#content)

1. **Between MDPs and Semi-MDPs: Learning, Planning, and Representing Knowledge at Multiple Temporal Scales, 1999** [Link](https://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1212&context=cs_faculty_pubs)

    *Richard Sutton, Doina Precup, Satinder Singh* 
    
2. **Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition, 2000** [Link](https://arxiv.org/pdf/cs/9905014.pdf)

    *Tom Dietterich* 
    
3. **The Promise of Hierarchical Reinforcement Learning, 2019** [Link](https://thegradient.pub/the-promise-of-hierarchical-reinforcement-learning/)

    *Yannis Flet-Berliac*
    
4. **The Option-Critic Architecture, 2016** [Link](https://arxiv.org/abs/1609.05140)

    *Pierre-Luc Bacon, Jean Harb, Doina Precup*
    
    Notes: I have been able to replicate the results using the pytorch implementation available here:  [Link](https://github.com/lweitkamp/option-critic-pytorch). I've implemented in the pacman environment as well. The results for Pacman are not good yet.
    
5. **From Skills to Symbols: Learning Symbolic Representations for Abstract High-Level Planning, 2018** [Link](https://cs.brown.edu/people/gdk/pubs/orig_sym_jair.pdf)

    *George Konidaris, Leslie Pack Kaelbling, Tomas Lozano-Perez*

## [GNN+RL](#content)
1. **NerveNet: Learning Structured Policy with Graph Neural Networks , 2018**[Link](https://openreview.net/forum?id=S1sqHMZCb)

*Tingwu Wang, Renjie Liao, Jimmy Ba, Sanja Fidler*

2. **Hierarchical Representations and Explicit Memory: Learning Effective Navigation Policies on 3D Scene Graphs using Graph Neural Networks, 2021** [Link](https://arxiv.org/abs/2108.01176)

    *Zachary Ravichandran, Lisa Peng, Nathan Hughes, J. Daniel Griffith, Luca Carlone* 

## [Various](#content)
