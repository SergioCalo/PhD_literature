# Literature:



** Denotes highly relevant paper
## [Content](#content)

<table>
<tr><td colspan="2"><a href="#cognitive-science">1. Cognitive Science</a></td></tr> 
<tr><td colspan="2"><a href="#state-representation-and-abstraction">2. State Representation and Abstraction</a></td></tr>
<tr><td colspan="2"><a href="#temporal-and-state-abstraction"> 2.1 Temporal and state abstraction</a></td></tr>
<tr><td colspan="2"><a href="#symbolic-representation-and-planning"> 2.2 Symbolic Representation and planning</a></td></tr>
<tr><td colspan="2"><a href="#hierarchical-reinforcement-learning">3. Hierarchical Reinforcement Learning</a></td></tr>
<tr><td colspan="2"><a href="#gnnrl">4. GNN+RL</a></td></tr>
<tr><td colspan="2"><a href="#various">5. Various</a></td></tr>

</table>

## [Cognitive Science](#content)
1. **Reward is enough, 2021** [Link](https://www.sciencedirect.com/science/article/pii/S0004370221000862)


    *David Silver, Satinder Singh, Doina Precup, Richard S.Sutton* 
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>


2. **Hierarchical principles of embodied reinforcement learning: A review, 2022** [Link](https://arxiv.org/abs/2012.10147)

    *Manfred Eppe, Christian Gumbsch, Matthias Kerzel, Phuong D.H. Nguyen, Martin V. Butz, Stefan Wermter* 
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>
    
    
## [State Representation and Abstraction](#content)

1. **Planning to Explore via Self-Supervised World Models, 2022** [Link](https://arxiv.org/abs/2005.05960)

    *Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, Deepak Pathak* 
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>

2. **Reinforcement Learning with Prototypical Representations, 2021** [Link](https://arxiv.org/abs/2102.11271)

     *Denis Yarats, Rob Fergus, Alessandro Lazaric, Lerrel Pinto* 
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> I have been able to replicate the results. </a>
  </ol>
</details>

    
 3. **Near Optimal Behavior via Approximate State Abstraction, 2016** [Link](https://arxiv.org/abs/1701.04113)

    *David Abel, D. Ellis Hershkowitz, Michael L. Littman* 
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>

 4. **On the necessity of abstraction, 2019** [Link](https://www.sciencedirect.com/science/article/pii/S2352154618302080)

    *George Konidaris* 
    
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> Opinion paper </a>
  </ol>
</details>
    
5. **The value of abstraction, 2016** [Link](https://markkho.github.io/documents/cobs2019_value_of_abstr.pdf)

    *Mark K. Ho, David Abel, Thomas L. Griffiths and Michael L. Littman* 
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> Opinion paper </a>
  </ol>
</details>
        
6. **A Theory of Abstraction in Reinforcement Learning, 2020** [Link](https://david-abel.github.io/thesis.pdf)

    *David Abel* 
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> PhD Thesis </a>
  </ol>
</details>
    
7. **Learning Markov State Abstractions for Deep Reinforcement Learning, 2021** [Link](https://arxiv.org/abs/2106.04379) **

    *Cameron Allen, Neev Parikh, Omer Gottesman, George Konidaris* 
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>

8. **State Abstractions for Lifelong Reinforcement Learning, 2018** [Link](https://arxiv.org/abs/2106.04379) **

    *David Abel, Dilip Arumugam, Lucas Lehnert, Michael L. Littman* 

<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>

9. **DeepMDP: Learning Continuous Latent Space Models for Representation Learning, 2019** [Link](https://arxiv.org/pdf/1906.02736.pdf) 

    *Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, Marc G. Bellemare* 
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>

10. **Learning Invariant Representations for Reinforcement Learning without Reconstruction, 2019** [Link](https://arxiv.org/abs/2006.10742) 

    *Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, Sergey Levine* 
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>

### [Temporal and state abstraction](#content)
      
 1. **Temporal Abstraction in Reinforcement Learning with the Successor Representation, 2022** [Link](https://arxiv.org/abs/2110.05740) **

    *Marlos C. Machado, Andre Barreto, Doina Precup, Michael Bowling*

<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>

 2. **Discrete State-Action Abstraction via the Successor Representation, 2023** [Link](https://arxiv.org/abs/2206.03467)

    *Amnon Attali, Pedro Cisneros-Velarde, Marco Morales, Nancy M. Amato*
 
 <details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>

3. **Deep Laplacian-based Options for Temporally-Extended Exploration, 2023** [Link](https://arxiv.org/pdf/2301.11181.pdf)

    *Martin Klissarov, Marlos C. Machado*
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>

4. **Successor Options: An Option Discovery Framework for Reinforcement Learning, 2019** [Link](https://arxiv.org/pdf/1905.05731.pdf)

    *Manan Tomar, Rahul Ramesh, Balaraman Ravindran*
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>

5. **Context-Specific Representation Abstraction for Deep Option Learning, 2022** [Link](https://arxiv.org/pdf/2109.09876.pdf)

    *Marwa Abdulhai, Dong-Ki Kim, Matthew Riemer, Miao Liu, Gerald Tesauro, Jonathan P. How*
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>
    
    
6. **Hierarchical reinforcement learning for efficient exploration and transfer, 2020** [Link](https://arxiv.org/pdf/2011.06335.pdf) **

    *Lorenzo Steccanella, Simone Totaro, Damien Allonsius, Anders Jonsson*
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> In this work, an invariant state space that is common to a range of tasks is compressed into a partition. It's assumed that the agent has access to the partition. </a>
  </ol>
</details>

7. **Near-Optimal Representation Learning for Hierarchical Reinforcement Learning, 2019** [Link](https://arxiv.org/abs/1810.01257) **

    *Ofir Nachum, Shixiang Gu, Honglak Lee, Sergey Levine*
    
<details>
  <summary> Comments </summary>
  <ol>
    <a>  </a>
  </ol>
</details>


### [Symbolic Representation and planning](#content)

1. **Learning State and Action Abstractions for Effective and Efficient Planning, 2020** [Link](https://dspace.mit.edu/handle/1721.1/145150)

    *Rohan Chitnis* 
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>

2. **From Skills to Symbols: Learning Symbolic Representations for Abstract High-Level Planning, 2018** [Link](https://cs.brown.edu/people/gdk/pubs/orig_sym_jair.pdf)

    *George Konidaris, Leslie Pack Kaelbling, Tomas Lozano-Perez*
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>

3. **Discovering State and Action Abstractions for Generalized Task and Motion Planning, 2022** [Link](https://cs.brown.edu/people/gdk/pubs/orig_sym_jair.pdf)

    *Aidan Curtis, Tom Silver, Joshua B. Tenenbaum, Tomas Lozano-PÃ©rez, Leslie Kaelbling*
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>

## [Hierarchical Reinforcement Learning](#content)

1. **Between MDPs and Semi-MDPs: Learning, Planning, and Representing Knowledge at Multiple Temporal Scales, 1999** [Link](https://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1212&context=cs_faculty_pubs)

    *Richard Sutton, Doina Precup, Satinder Singh* 
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>    

2. **Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition, 2000** [Link](https://arxiv.org/pdf/cs/9905014.pdf)

    *Tom Dietterich* 
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>    

3. **The Promise of Hierarchical Reinforcement Learning, 2019** [Link](https://thegradient.pub/the-promise-of-hierarchical-reinforcement-learning/)

    *Yannis Flet-Berliac*
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>    

4. **The Option-Critic Architecture, 2016** [Link](https://arxiv.org/abs/1609.05140)

    *Pierre-Luc Bacon, Jean Harb, Doina Precup*

<details>
  <summary> Comments </summary>
  <ol>
    <a> I have been able to replicate the results using the pytorch implementation available here:  [Link](https://github.com/lweitkamp/option-critic-pytorch). I've implemented in the pacman environment as well. The results for Pacman are not good yet. </a>
  </ol>
</details>
    

    
5. **HIERARCHICAL REINFORCEMENT LEARNING IN CONTINUOUS STATE AND MULTI-AGENT ENVIRONMENTS, 2005** [Link](http://all.cs.umass.edu/pubs/2005/ghavamzadeh_thesis05.pdf)

    *MOHAMMAD GHAVAMZADEH*
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> PhD thesis </a>
  </ol>
</details> 
    
6. **Induction and Exploitation of Subgoal Automata for Reinforcement Learning, 2021** [Link](https://arxiv.org/pdf/2009.03855.pdf)

    *Daniel Furelos-Blanco, Mark Law, Anders Jonsson, Krysia Broda, Alessandra Russo*
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>    
    

## [GNN+RL](#content)
1. **NerveNet: Learning Structured Policy with Graph Neural Networks , 2018**[Link](https://openreview.net/forum?id=S1sqHMZCb)

*Tingwu Wang, Renjie Liao, Jimmy Ba, Sanja Fidler*

<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details> 

2. **Hierarchical Representations and Explicit Memory: Learning Effective Navigation Policies on 3D Scene Graphs using Graph Neural Networks, 2021** [Link](https://arxiv.org/abs/2108.01176)

    *Zachary Ravichandran, Lisa Peng, Nathan Hughes, J. Daniel Griffith, Luca Carlone* 
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details>    

3. **Reward Propagation Using Graph Convolutional Networks, 2020** [Link](https://proceedings.neurips.cc/paper/2020/file/970627414218ccff3497cb7a784288f5-Paper.pdf)

    *Martin Klissarov, Doina Precup* 
    
<details>
  <summary> Comments </summary>
  <ol>
    <a> No comments available yet </a>
  </ol>
</details> 
    
    

## [Various](#content)
